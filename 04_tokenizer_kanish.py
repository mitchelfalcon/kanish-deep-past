import re

class KanishTokenizer:
    """
    Tokenizador especializado para textos cuneiformes (Old Assyrian).
    Separa morfemas gramaticales (-ma, -ni) sin romper logogramas.
    """
    
    def __init__(self):
        # Lista de cl칤ticos comunes en Kanesh para separar
        # -ma: Enf치tico / Conectivo
        # -ni: Subjuntivo
        # -kum/코um: Dativo
        # -am: Ventivo
        self.cliticos = [
            'ma', 'ni', 'kum', '코um', 'am', 'kunu', '코unu', 'ka', 'su'
        ]
        
        # Regex compilado para velocidad
        # Busca un guion seguido de un cl칤tico, pero SOLO si es el final de la palabra (\b)
        self.regex_cliticos = re.compile(r'-(' + '|'.join(self.cliticos) + r')\b', re.IGNORECASE)

    def tokenizar(self, texto):
        if not texto or not isinstance(texto, str):
            return []

        texto = texto.strip()

        # PASO 1: Protecci칩n de Logogramas complejos (Heur칤stica)
        # Si hay puntos (DUMU.ZI) asumimos que es un logograma y no lo tocamos por ahora.
        # (En versiones futuras, esto se conecta al Grafo para validar entidades)

        # PASO 2: Separaci칩n Quir칰rgica de Cl칤ticos
        # Transforma "iqbi-ma" en "iqbi -ma"
        texto_procesado = self.regex_cliticos.sub(r' -\1', texto)
        
        # PASO 3: Split est치ndar por espacios
        tokens = texto_procesado.split()
        
        return tokens

# --- BLOQUE DE PRUEBA R츼PIDA ---
if __name__ == "__main__":
    tk = KanishTokenizer()
    ejemplos = [
        "um-ma En-lil-ba-ni-ma",       # Nombre propio + cl칤tico -ma
        "k칯-babbar i-di-in-코um",       # Verbo + cl칤tico dativo -코um
        "DUMU.ZI i-li-ik",             # Logograma con punto (no debe separarse)
        "a-na b캶t kar-im",             # Preposici칩n y sustantivo
        "[x ... ] <BROKEN>"            # Token de rotura (del paso anterior)
    ]
    
    print("--- 游빍 PRUEBA DE TOKENIZACI칍N ---")
    for ej in ejemplos:
        print(f"ORIGINAL: {ej}")
        print(f"TOKENS:   {tk.tokenizar(ej)}")
        print("-" * 30)